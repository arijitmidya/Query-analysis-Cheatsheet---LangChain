{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to handle multiple queries when doing query analysis\n",
        "\n",
        "When using LangChain to analyze user input and generate search queries, our analysis might sometimes result in multiple queries being created. This can happen when the user's request is complex and requires breaking it down into several smaller, more specific searches.\n",
        "\n",
        "In these cases, our LangChain workflow needs to be designed to handle multiple queries efficiently. We need to:\n",
        "\n",
        "Execute each of the generated queries.\n",
        "Combine the results from all the queries into a single, coherent response.\n",
        "We'll demonstrate how to do this using simulated data. This will show how LangChain can be used to manage and process multiple queries, ensuring that all relevant information is retrieved and presented to the user.\n",
        "\n",
        "Essentially, we're building LangChain workflows that are capable of managing and orchestrating multiple retrieval operations and combining the results into an accurate response.\n"
      ],
      "metadata": {
        "id": "8Uf2bpJcbGAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "WIgrgIrvSJJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain langchain-community langchain-openai langchain-chroma"
      ],
      "metadata": {
        "id": "j5IM84g4R-SN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "JhiUI2TTSHDq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Index\n",
        "\n",
        "This code does the following:\n",
        "\n",
        "It takes a simple text string.\n",
        "\n",
        "It uses OpenAI's \"text-embedding-3-small\" model to generate a numerical representation (embedding) of that string.\n",
        "\n",
        "It stores the string and its embedding in a Chroma vector database.\n",
        "\n",
        "It creates a retriever that can be used to find similar texts based on their embeddings."
      ],
      "metadata": {
        "id": "Et2V24TcedsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "texts = [\"Harrison worked at Kensho\", \"Ankush worked at Facebook\"]\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts,\n",
        "    embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "Y7jogrKeZwbf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Query analysis\n",
        "\n",
        "a. Import necessary dependencies\n",
        "\n",
        "b. Search Class Definition\n",
        "\n",
        "class Search(BaseModel):: Defines a new class named Search that inherits from BaseModel. This makes it a Pydantic model.\n",
        "\n",
        "c. queries Field Definition:\n",
        "\n",
        "queries: List[str] = Field(..., description=\"Distinct queries to search for\"): Defines a field named queries within the Search model.\n",
        "\n",
        "queries: List[str]: Specifies that the queries field should be a list of strings.\n",
        "\n",
        "Field(...): Uses the Field function to configure the field.\n",
        "\n",
        "...: The ellipsis (...) indicates that this field is required. If a Search object is created without a value for queries, Pydantic will raise a validation error.\n",
        "\n",
        "description=\"Distinct queries to search for\": Sets the description of the queries field. This description is useful for documentation and generating schemas.\n",
        "\n"
      ],
      "metadata": {
        "id": "V-UtMtPxev_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search over a database of job records.\"\"\"\n",
        "\n",
        "    queries: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"Distinct queries to search for\",\n",
        "    )"
      ],
      "metadata": {
        "id": "ERnLjufqelBr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design the langchain chain\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. Output Parser Creation:\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search]): Creates a PydanticToolsParser object.\n",
        "\n",
        "tools=[Search]: Specifies that the parser should parse tool calls that match the Search Pydantic model (which, in this case, allows for a list of search queries).\n",
        "\n",
        "3. System Prompt Definition:\n",
        "\n",
        "system = \"\"\"...\"\"\": Defines a system prompt for the language model.\n",
        "\n",
        "It instructs the model that it can issue search queries.\n",
        "\n",
        "Crucially, it explicitly allows the model to perform two distinct searches if needed.\n",
        "\n",
        "4. Chat Prompt Template Creation:\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([...]): Creates a chat prompt template.\n",
        "\n",
        "(\"system\", system): Adds the system prompt.\n",
        "\n",
        "(\"human\", \"{question}\"): Adds a placeholder for the user's question.\n",
        "\n",
        "5. Language Model Initialization:\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0): Initializes a ChatOpenAI object.\n",
        "\n",
        "model=\"gpt-4o-mini\": Specifies the OpenAI chat model.\n",
        "\n",
        "temperature=0: Sets the temperature to 0, making the model's responses more deterministic.\n",
        "\n",
        "6. Structured LLM Creation:\n",
        "\n",
        "structured_llm = llm.with_structured_output(Search): Creates a version of the LLM that is configured to return structured output matching the Search Pydantic model.\n",
        "\n",
        "with_structured_output(Search): This LangChain method enables the LLM to directly output a pydantic object, in this case, a Search object. This is an alternative to using tool calling.\n",
        "\n",
        "7. Query Analyzer Chain Creation:\n",
        "\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm: Creates a runnable chain.\n",
        "\n",
        "{\"question\": RunnablePassthrough()}: Passes the user's question through.\n",
        "| prompt: Formats the question with the system prompt.\n",
        "| structured_llm: Sends the formatted prompt to the language model, which generates a structured Search object as output.\n",
        "\n",
        "In essence:\n",
        "\n",
        "This code sets up a pipeline that:\n",
        "\n",
        "Takes a user question as input.\n",
        "\n",
        "Formats the question with a system prompt that encourages the language model to use search queries, and to use multiple queries if necessary.\n",
        "\n",
        "Sends the formatted prompt to the language model, which is configured to return a structured Search object (containing a list of search queries).\n",
        "\n",
        "Returns a pydantic object of type search, that contains a list of strings.\n",
        "\n",
        "This pipeline is designed to handle user questions that might require multiple searches to answer, making it more flexible than the previous example. The major difference is that this implementation uses with_structured_output instead of tool calling."
      ],
      "metadata": {
        "id": "9F4VPtihymxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search])\n",
        "\n",
        "system = \"\"\"You have the ability to issue search queries to get information to help answer user information.\n",
        "\n",
        "If you need to look up two distinct pieces of information, you are allowed to do that!\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(Search)\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "id": "Ke0vZlPBe0f3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1x23jFQe3JF",
        "outputId": "795fae03-1c19-41b0-b46e-54b7d9924b1a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(queries=['Harrison Work history', 'Harrison employment records'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"where did Harrison and ankush Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpK2TIzI3S3v",
        "outputId": "99b7e866-df64-4dff-ad6b-0ed8369f47f9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(queries=['Harrison work history', 'Ankush work history'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Retrieval with query analysis"
      ],
      "metadata": {
        "id": "azPfA2wFe97_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. @chain Decorator:\n",
        "\n",
        "@chain: This decorator makes the custom_chain function a LangChain runnable, allowing it to be used in LangChain's runnable framework.\n",
        "\n",
        "3. custom_chain Asynchronous Function Definition:\n",
        "\n",
        "async def custom_chain(question):: Defines an asynchronous function custom_chain.\n",
        "async: Indicates that this function can use await to pause execution until asynchronous operations complete.\n",
        "question: Takes a question (presumably a string) as input.\n",
        "\n",
        "4. Asynchronous Query Analysis:\n",
        "\n",
        "response = await query_analyzer.ainvoke(question): This line asynchronously invokes the query_analyzer chain (defined in a previous example) with the input question.\n",
        "ainvoke(): This is the asynchronous version of invoke(), used for asynchronous runnables.\n",
        "await: Pauses execution until the query_analyzer completes and returns a result.\n",
        "The result (a Search object containing a list of queries) is stored in the response variable.\n",
        "\n",
        "5. Document Retrieval Loop:\n",
        "\n",
        "docs = []: Initializes an empty list named docs to store the retrieved documents.\n",
        "for query in response.queries:: Iterates through the list of queries in the response.queries attribute.\n",
        "new_docs = await retriever.ainvoke(query): Asynchronously invokes the retriever (defined in a previous example) with the current query.\n",
        "ainvoke(): Asynchronous invocation of the retriever.\n",
        "await: Pauses execution until the retriever completes and returns a list of documents.\n",
        "docs.extend(new_docs): Extends the docs list with the retrieved documents.\n",
        "\n",
        "6. Document Reranking/Deduplication (Comment):\n",
        "\n",
        "# You probably want to think about reranking or deduplicating documents here: This comment reminds you that after retrieving documents from multiple queries, you might want to:\n",
        "Rerank: Reorder the documents based on relevance, potentially considering factors like the original question and the combined content of the documents.\n",
        "Deduplicate: Remove duplicate documents that might have been retrieved by different queries.\n",
        "# But that is a separate topic: This indicates that reranking and deduplication are beyond the scope of this code snippet.\n",
        "\n",
        "7. Return Documents:\n",
        "\n",
        "return docs: Returns the list of retrieved documents.\n",
        "In essence:\n",
        "\n",
        "The custom_chain function does the following:\n",
        "\n",
        "It takes a user question as input.\n",
        "\n",
        "It asynchronously uses the query_analyzer to generate a list of search queries.\n",
        "\n",
        "It iterates through the list of queries, asynchronously retrieving documents for each query using the retriever.\n",
        "\n",
        "It combines the retrieved documents into a single list.\n",
        "\n",
        "It returns the combined list of documents.\n",
        "\n",
        "This chain is designed to handle user questions that might require multiple searches, retrieving documents for each search and combining the results. It also highlights the need for further processing (reranking and deduplication) in a real-world application."
      ],
      "metadata": {
        "id": "8HFVY26I0Y6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "@chain\n",
        "async def custom_chain(question):\n",
        "    response = await query_analyzer.ainvoke(question)\n",
        "    docs = []\n",
        "    for query in response.queries:\n",
        "        new_docs = await retriever.ainvoke(query)\n",
        "        docs.extend(new_docs)\n",
        "    # You probably want to think about reranking or deduplicating documents here\n",
        "    # But that is a separate topic\n",
        "    return docs"
      ],
      "metadata": {
        "id": "Fbew4sM6fBes"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await custom_chain.ainvoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-c3GcEzfDeD",
        "outputId": "d3908e45-a357-4d25-ea4a-d5347e0971f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a5e84727-c834-49f9-981b-040705761fb2', metadata={}, page_content='Harrison worked at Kensho'),\n",
              " Document(id='a5e84727-c834-49f9-981b-040705761fb2', metadata={}, page_content='Harrison worked at Kensho')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. await custom_chain.ainvoke(\"where did Harrison and ankush Work\"):\n",
        "\n",
        "This line asynchronously calls the custom_chain function with the question \"where did Harrison and ankush Work\".\n",
        "\n",
        "As we know from the previous explanation, this chain will:\n",
        "\n",
        "Use the query_analyzer to generate a list of search queries (in this case, likely two: \"where did Harrison work\" and \"where did Ankush work\").\n",
        "\n",
        "Asynchronously retrieve documents for each query using the retriever.\n",
        "Combine the retrieved documents into a single list.\n",
        "\n",
        "Return the list of documents."
      ],
      "metadata": {
        "id": "n8GAuLsT6csC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await custom_chain.ainvoke(\"where did Harrison and ankush Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiUS6qk38KN",
        "outputId": "db2408ec-faf7-4b23-cf5f-fcf9047044bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a5e84727-c834-49f9-981b-040705761fb2', metadata={}, page_content='Harrison worked at Kensho'),\n",
              " Document(id='2e01cd83-33c9-4b78-9802-f2787717c16f', metadata={}, page_content='Ankush worked at Facebook')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output explanation\n",
        "\n",
        "This is the list of documents returned by the custom_chain.\n",
        "\n",
        "It contains two Document objects, indicating that the chain successfully retrieved two relevant documents.\n",
        "\n",
        "Document(id='a5e84727-c834-49f9-981b-040705761fb2', metadata={}, page_content='Harrison worked at Kensho'):\n",
        "\n",
        "This document's page_content is \"Harrison worked at Kensho\", which answers the first part of the question.\n",
        "\n",
        "The id is a unique identifier assigned by Chroma.\n",
        "\n",
        "metadata is empty.\n",
        "\n",
        "Document(id='2e01cd83-33c9-4b78-9802-f2787717c16f', metadata={}, page_content='Ankush worked at Facebook'):\n",
        "\n",
        "This document's page_content is \"Ankush worked at Facebook\", which answers the second part of the question.\n",
        "\n",
        "The id is a unique identifier.\n",
        "\n",
        "metadata is empty."
      ],
      "metadata": {
        "id": "luWCIxOK1VYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi_6Io_4fHos",
        "outputId": "471ae2a1-70a9-4cce-f4a8-91eba3a739da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 93, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-33eada19-6f80-48eb-9e07-352003ba2b28-0', usage_metadata={'input_tokens': 93, 'output_tokens': 11, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"Who is david !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOx34-MfPI8",
        "outputId": "bcad4be8-57f0-4c6c-b79d-e243c29e2a7a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Could you please provide more context about which David you are referring to? There are many notable individuals named David, including historical figures, celebrities, and fictional characters.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 95, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eab6ba8-fa97-4abc-8dfe-fda4ec97ee41-0', usage_metadata={'input_tokens': 95, 'output_tokens': 34, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6ynNXp-1dPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}