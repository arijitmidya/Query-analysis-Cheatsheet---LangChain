{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to handle multiple retrievers when doing query analysis\n",
        "\n",
        "When using LangChain for complex information retrieval, our query analysis might determine that different types of information sources (retrievers) are needed to answer a user's question. For example, one part of the question might require searching a database, while another part might require searching a document repository.\n",
        "\n",
        "In these cases, our LangChain workflow needs to be able to dynamically choose the appropriate retriever based on the query analysis results. This requires adding logic to our chain that can:\n",
        "\n",
        "Examine the output of the query analysis.\n",
        "Select the correct retriever based on the identified information needs.\n",
        "Execute the retrieval using the chosen retriever.\n",
        "We'll demonstrate how to implement this using simulated data. This will show how LangChain can be used to build workflows that intelligently route queries to different retrievers, ensuring that the most relevant information is retrieved for each part of the user's request.\n",
        "\n",
        "Essentially, we're building LangChain workflows that can intelligently decide which information source (retriever) to use for each part of a complex query, enabling us to retrieve information from diverse sources in a unified way."
      ],
      "metadata": {
        "id": "8Uf2bpJcbGAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "WIgrgIrvSJJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain langchain-community langchain-openai langchain-chroma"
      ],
      "metadata": {
        "id": "j5IM84g4R-SN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "JhiUI2TTSHDq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Index\n",
        "\n",
        "This code creates two separate knowledge bases, each stored in its own Chroma collection. This is useful for:\n",
        "\n",
        "Isolating information: Each person's information is kept separate, which can be important for privacy or organization.\n",
        "\n",
        "Targeted retrieval: You can use the appropriate retriever to find information about a specific person.\n",
        "\n",
        "It also demonstrates the use of the search_kwargs parameter, which allows for customization of the search.\n",
        "\n",
        "The code effectively sets up two independent retrieval systems, allowing you to search for information about Harrison and Ankush separately."
      ],
      "metadata": {
        "id": "Et2V24TcedsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "texts = [\"Harrison worked at Kensho\"]\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = Chroma.from_texts(texts, embeddings, collection_name=\"harrison\")\n",
        "retriever_harrison = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "texts = [\"Ankush worked at Facebook\"]\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = Chroma.from_texts(texts, embeddings, collection_name=\"ankush\")\n",
        "retriever_ankush = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "Y7jogrKeZwbf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Query analysis\n",
        "\n",
        "a. Import necessary dependencies\n",
        "\n",
        "2. Search Class Definition:\n",
        "\n",
        "class Search(BaseModel):: Defines a new class named Search that inherits from BaseModel, making it a Pydantic model.\n",
        "\n",
        "3. query Field Definition:\n",
        "\n",
        "query: str = Field(..., description=\"Query to look up\"): Defines a field named query of type string.\n",
        "\n",
        "...: The ellipsis indicates that this field is required.\n",
        "description=\"Query to look up\": Provides a description of the field, useful for documentation and schema generation.\n",
        "\n",
        "4. person Field Definition:\n",
        "\n",
        "person: str = Field(..., description=\"Person to look things up for. Should beHARRISONorANKUSH.\"): Defines a field named person of type string.\n",
        "\n",
        "...: The ellipsis indicates that this field is required.\n",
        "\n",
        "description=\"Person to look things up for. Should beHARRISONorANKUSH.\": Provides a description of the field, including a constraint that it should be either \"HARRISON\" or \"ANKUSH\". This constraint isn't enforced by Pydantic itself (it's in the description, to inform the user), but it suggests that the code using this model will likely validate this constraint.\n",
        "\n"
      ],
      "metadata": {
        "id": "V-UtMtPxev_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search for information about a person.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"Query to look up\",\n",
        "    )\n",
        "    person: str = Field(\n",
        "        ...,\n",
        "        description=\"Person to look things up for. Should be `HARRISON` or `ANKUSH`.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "ERnLjufqelBr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design the langchain chain\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. Output Parser Creation:\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search]): Creates a PydanticToolsParser object.\n",
        "\n",
        "tools=[Search]: Specifies that the parser should parse tool calls that match the Search Pydantic model (which, in this case, allows for a specific query and a person name).\n",
        "\n",
        "3. System Prompt Definition:\n",
        "\n",
        "system = \"\"\"...\"\"\": Defines a system prompt for the language model.\n",
        "It instructs the model that it can issue search queries.\n",
        "\n",
        "4. Chat Prompt Template Creation:\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([...]): Creates a chat prompt template.\n",
        "\n",
        "(\"system\", system): Adds the system prompt.\n",
        "\n",
        "(\"human\", \"{question}\"): Adds a placeholder for the user's question.\n",
        "\n",
        "5. Language Model Initialization:\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0): Initializes a ChatOpenAI object.\n",
        "\n",
        "model=\"gpt-4o-mini\": Specifies the OpenAI chat model.\n",
        "\n",
        "temperature=0: Sets the temperature to 0, making the model's responses more deterministic.\n",
        "\n",
        "6. Structured LLM Creation:\n",
        "\n",
        "structured_llm = llm.with_structured_output(Search): Creates a version of the LLM that is configured to return structured output matching the Search Pydantic model.\n",
        "\n",
        "with_structured_output(Search): This LangChain method enables the LLM to directly output a pydantic object, in this case, a Search object.\n",
        "\n",
        "7. Query Analyzer Chain Creation:\n",
        "\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm: Creates a runnable chain.\n",
        "\n",
        "{\"question\": RunnablePassthrough()}: Passes the user's question through.\n",
        "| prompt: Formats the question with the system prompt.\n",
        "\n",
        "| structured_llm: Sends the formatted prompt to the language model, which generates a structured Search object as output.\n",
        "\n",
        "In essence:\n",
        "\n",
        "This code sets up a pipeline that:\n",
        "\n",
        "Takes a user question as input.\n",
        "\n",
        "Formats the question with a system prompt that encourages the language model to use search queries.\n",
        "\n",
        "Sends the formatted prompt to the language model, which is configured to return a structured Search object (containing a query and a person name).\n",
        "\n",
        "Returns a pydantic object of type search.\n",
        "\n",
        "This pipeline is designed to handle user questions that require searches about a specific person, ensuring that the generated search queries include both the search term and the target person's name."
      ],
      "metadata": {
        "id": "9F4VPtihymxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search])\n",
        "\n",
        "system = \"\"\"You have the ability to issue search queries to get information to help answer user information.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(Search)\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "id": "Ke0vZlPBe0f3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1x23jFQe3JF",
        "outputId": "40de403e-cb3c-44d4-d025-6e4b1cb859d3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='Harrison work history', person='HARRISON')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"where did ankush Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpK2TIzI3S3v",
        "outputId": "02ac950a-4710-461b-88a7-b0cd0ada6fab"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='Ankush work history', person='ANKUSH')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Retrieval with query analysis"
      ],
      "metadata": {
        "id": "azPfA2wFe97_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. @chain Decorator:\n",
        "\n",
        "@chain: This decorator makes the custom_chain function a LangChain runnable, allowing it to be used in LangChain's runnable framework.\n",
        "\n",
        "3. custom_chain Asynchronous Function Definition:\n",
        "\n",
        "async def custom_chain(question):: Defines an asynchronous function custom_chain.\n",
        "async: Indicates that this function can use await to pause execution until asynchronous operations complete.\n",
        "question: Takes a question (presumably a string) as input.\n",
        "\n",
        "4. Asynchronous Query Analysis:\n",
        "\n",
        "response = await query_analyzer.ainvoke(question): This line asynchronously invokes the query_analyzer chain (defined in a previous example) with the input question.\n",
        "ainvoke(): This is the asynchronous version of invoke(), used for asynchronous runnables.\n",
        "await: Pauses execution until the query_analyzer completes and returns a result.\n",
        "The result (a Search object containing a list of queries) is stored in the response variable.\n",
        "\n",
        "5. Document Retrieval Loop:\n",
        "\n",
        "docs = []: Initializes an empty list named docs to store the retrieved documents.\n",
        "for query in response.queries:: Iterates through the list of queries in the response.queries attribute.\n",
        "new_docs = await retriever.ainvoke(query): Asynchronously invokes the retriever (defined in a previous example) with the current query.\n",
        "ainvoke(): Asynchronous invocation of the retriever.\n",
        "await: Pauses execution until the retriever completes and returns a list of documents.\n",
        "docs.extend(new_docs): Extends the docs list with the retrieved documents.\n",
        "\n",
        "6. Document Reranking/Deduplication (Comment):\n",
        "\n",
        "# You probably want to think about reranking or deduplicating documents here: This comment reminds you that after retrieving documents from multiple queries, you might want to:\n",
        "Rerank: Reorder the documents based on relevance, potentially considering factors like the original question and the combined content of the documents.\n",
        "Deduplicate: Remove duplicate documents that might have been retrieved by different queries.\n",
        "# But that is a separate topic: This indicates that reranking and deduplication are beyond the scope of this code snippet.\n",
        "\n",
        "7. Return Documents:\n",
        "\n",
        "return docs: Returns the list of retrieved documents.\n",
        "In essence:\n",
        "\n",
        "The custom_chain function does the following:\n",
        "\n",
        "It takes a user question as input.\n",
        "\n",
        "It asynchronously uses the query_analyzer to generate a list of search queries.\n",
        "\n",
        "It iterates through the list of queries, asynchronously retrieving documents for each query using the retriever.\n",
        "\n",
        "It combines the retrieved documents into a single list.\n",
        "\n",
        "It returns the combined list of documents.\n",
        "\n",
        "This chain is designed to handle user questions that might require multiple searches, retrieving documents for each search and combining the results. It also highlights the need for further processing (reranking and deduplication) in a real-world application."
      ],
      "metadata": {
        "id": "8HFVY26I0Y6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "retrievers = {\n",
        "    \"HARRISON\": retriever_harrison,\n",
        "    \"ANKUSH\": retriever_ankush,\n",
        "}\n",
        "\n",
        "@chain\n",
        "def custom_chain(question):\n",
        "    response = query_analyzer.invoke(question)\n",
        "    retriever = retrievers[response.person]\n",
        "    return retriever.invoke(response.query)"
      ],
      "metadata": {
        "id": "Fbew4sM6fBes"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-c3GcEzfDeD",
        "outputId": "ce96dcc6-2011-467f-eda7-64f2ca531542"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='1f0c09b3-77af-482e-8c7e-9ca3ed7530b3', metadata={}, page_content='Harrison worked at Kensho')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"where did ankush Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiUS6qk38KN",
        "outputId": "ce5a5d4a-8112-4e04-a876-6668aa6e2932"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a64dfaa7-b228-4976-86df-659b07999fb0', metadata={}, page_content='Ankush worked at Facebook')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6ynNXp-1dPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}