{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to handle cases where no queries are generated\n",
        "\n",
        "When using LangChain to analyze user input and generate search queries, sometimes the analysis might determine that no specific search query is needed. This can happen when the user's input is not a direct request for information retrieval, or when the analysis determines that the user is asking a question that is already known and doesn't require a database query.\n",
        "\n",
        "In these situations, our LangChain workflow needs to be smart enough to handle the case where no queries are generated. We need to check the output of the query analysis step and decide whether to proceed with retrieving information from our data source (retriever) or to handle the situation in a different way.\n",
        "\n",
        "To demonstrate this, we'll use simulated data to show how LangChain can gracefully handle scenarios where no queries are generated, and how we can add logic to our chain to make decisions based on the query analysis results.\n",
        "\n",
        "Essentially, we're building LangChain workflows that are robust enough to handle all possible outputs of the query analysis, including the case where no queries are deemed necessary for retrieval."
      ],
      "metadata": {
        "id": "8Uf2bpJcbGAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "WIgrgIrvSJJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain langchain-community langchain-openai langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5IM84g4R-SN",
        "outputId": "79dee533-43fe-47d9-84ab-3eb8290b3079"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "JhiUI2TTSHDq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Index\n",
        "\n",
        "This code does the following:\n",
        "\n",
        "It takes a simple text string.\n",
        "\n",
        "It uses OpenAI's \"text-embedding-3-small\" model to generate a numerical representation (embedding) of that string.\n",
        "\n",
        "It stores the string and its embedding in a Chroma vector database.\n",
        "\n",
        "It creates a retriever that can be used to find similar texts based on their embeddings."
      ],
      "metadata": {
        "id": "Et2V24TcedsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "texts = [\"Harrison worked at Kensho\"]\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts,\n",
        "    embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "Y7jogrKeZwbf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Query analysis\n",
        "\n",
        "a. Import necessary dependencies\n",
        "\n",
        "b. Search Class Definition\n",
        "\n",
        "class Search(BaseModel):: Defines a new class named Search that inherits from BaseModel. This makes it a Pydantic model.\n",
        "\n",
        "c. query Field Definition\n",
        "\n",
        "query: str = Field(..., description=\"Similarity search query applied to job record.\"): Defines a field named query within the Search model.\n",
        "\n",
        "query: str: Specifies that the query field should be a string.\n",
        "\n",
        "Field(...): Uses the Field function to configure the field.\n",
        "\n",
        "...: The ellipsis (...) as the first argument indicates that this field is required. If a Search object is created without a value for query, Pydantic will raise a validation error.\n",
        "\n",
        "description=\"Similarity search query applied to job record.\": Sets the description of the query field. This description is useful for documentation, generating schemas, and providing helpful error messages.\n",
        "\n"
      ],
      "metadata": {
        "id": "V-UtMtPxev_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search over a database of job records.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to job record.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "ERnLjufqelBr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design the langchain chain\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. System Prompt Definition:\n",
        "\n",
        "system = \"\"\"...\"\"\": Defines a system prompt, a crucial part of chat-based interactions.\n",
        "\n",
        "This prompt instructs the language model on its role:\n",
        "\n",
        "It can issue search queries.\n",
        "\n",
        "It should only use search when necessary.\n",
        "\n",
        "It should respond normally if search isn't needed.\n",
        "\n",
        "3. Chat Prompt Template Creation:\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([...]): Creates a chat prompt template.\n",
        "\n",
        "(\"system\", system): Adds the system prompt to the template.\n",
        "\n",
        "(\"human\", \"{question}\"): Adds a human message placeholder, where the user's question will be inserted.\n",
        "\n",
        "4. Language Model Initialization:\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0): Initializes a ChatOpenAI object.\n",
        "\n",
        "model=\"gpt-4o-mini\": Specifies the OpenAI chat model to use.\n",
        "\n",
        "temperature=0: Sets the temperature to 0, making the model's responses more deterministic (less random).\n",
        "\n",
        "5. Tool Binding:\n",
        "\n",
        "structured_llm = llm.bind_tools([Search]): Binds the Search tool to the language model.\n",
        "\n",
        "bind_tools([Search]): Enables the language model to use the Search tool when it determines it's necessary.\n",
        "\n",
        "Search is assumed to be a Pydantic model (as seen in your previous question) that defines the structure for search queries.\n",
        "\n",
        "This sets up the language model to be able to output a correctly formatted Search object.\n",
        "\n",
        "6. Query Analyzer Chain Creation:\n",
        "\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm: Creates a runnable chain.\n",
        "\n",
        "{\"question\": RunnablePassthrough()}: Takes the input (which is expected to be a dictionary with a \"question\" key) and passes the question value through.\n",
        "\n",
        "| prompt: Passes the question to the chat prompt template, which combines it with the system prompt.\n",
        "\n",
        "| structured_llm: Passes the formatted prompt to the language model, which generates a response, potentially including a Search tool call.\n",
        "\n",
        "Summary :\n",
        "\n",
        "This code sets up a pipeline that:\n",
        "\n",
        "1. Takes a user question as input.\n",
        "\n",
        "2. Formats the question with a system prompt that encourages the language model to use search when appropriate.\n",
        "\n",
        "3. Sends the formatted prompt to the language model, which is capable of generating responses and/or tool calls (specifically, Search tool calls).\n",
        "\n",
        "4. Returns the response from the language model, which could include a structured search query.\n",
        "\n",
        "This setup is useful for building systems that can intelligently decide whether to perform a search to answer a user's question, or just answer it directly."
      ],
      "metadata": {
        "id": "9F4VPtihymxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "system = \"\"\"You have the ability to issue search queries to get information to help answer user information.\n",
        "\n",
        "You do not NEED to look things up. If you don't need to, then just respond normally.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm = llm.bind_tools([Search])\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "id": "Ke0vZlPBe0f3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1x23jFQe3JF",
        "outputId": "6887c34b-49c6-4909-893c-09606a097c5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z8UHUeC9BbbbvNRKoQPjlC5i', 'function': {'arguments': '{\"query\":\"Harrison Work\"}', 'name': 'Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 95, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eeb630ed-be4e-4d2f-a07a-0f6d673229b0-0', tool_calls=[{'name': 'Search', 'args': {'query': 'Harrison Work'}, 'id': 'call_z8UHUeC9BbbbvNRKoQPjlC5i', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 16, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Retrieval with query analysis"
      ],
      "metadata": {
        "id": "azPfA2wFe97_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. Output Parser Creation:\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search]): Creates a PydanticToolsParser object.\n",
        "\n",
        "tools=[Search]: Specifies the Pydantic models that the parser should use to parse tool calls. In this case, it's the Search model (which you defined in a previous example).\n",
        "\n",
        "This means that the parser is configured to expect tool calls that match the structure defined in the Search model and to convert those tool calls into instances of the Search class."
      ],
      "metadata": {
        "id": "UZHlHWWpz9z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Search])"
      ],
      "metadata": {
        "id": "mIwiPP1Me5m2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. @chain Decorator:\n",
        "\n",
        "@chain: This decorator from LangChain transforms the custom_chain function into a runnable chain. This allows it to be used within LangChain's runnable framework.\n",
        "\n",
        "2. custom_chain Function Definition:\n",
        "\n",
        "def custom_chain(question):: Defines the function custom_chain that takes a question (presumably a string) as input.\n",
        "\n",
        "3. query_analyzer.invoke(question):\n",
        "\n",
        "response = query_analyzer.invoke(question): This line invokes the query_analyzer chain (defined in a previous example) with the input question.\n",
        "The query_analyzer chain is designed to analyze the question and potentially generate a tool call (specifically, a Search tool call) if it determines that a search is necessary.\n",
        "The result of invoking the query_analyzer chain is stored in the response variable.\n",
        "\n",
        "4. Tool Call Check:\n",
        "\n",
        "if \"tool_calls\" in response.additional_kwargs:: This line checks if the response from the query_analyzer chain contains tool calls.\n",
        "response.additional_kwargs: This attribute of the response object holds additional information, including tool calls.\n",
        "If the response contains tool calls, it means the query_analyzer determined that a search is necessary.\n",
        "\n",
        "5. Tool Call Processing (if tool calls exist):\n",
        "\n",
        "query = output_parser.invoke(response): This line invokes the output_parser (defined in a previous example) with the response from the query_analyzer.\n",
        "\n",
        "The output_parser parses the tool calls from the response and converts them into Pydantic objects (in this case, Search objects).\n",
        "\n",
        "The result of the parsing (a list of Search objects) is stored in the query variable.\n",
        "\n",
        "docs = retriever.invoke(query[0].query): This line invokes the retriever (defined in a previous example) with the query extracted from the tool call.\n",
        "\n",
        "query[0].query: This accesses the query attribute of the first Search object in the query list. This is the search query string.\n",
        "\n",
        "The retriever performs a similarity search on the vector store and returns the most similar documents (or text chunks).\n",
        "\n",
        "The results of the retrieval are stored in the docs variable.\n",
        "\n",
        "# Could add more logic - like another LLM call - here: This is a comment indicating that you could add more logic here, such as another language model call to process the retrieved documents.\n",
        "\n",
        "return docs: This line returns the retrieved documents.\n",
        "\n",
        "6. No Tool Call Handling (if no tool calls exist):\n",
        "\n",
        "else: return response: If the response from the query_analyzer chain does not contain tool calls, this line returns the response directly. This means the query_analyzer determined that a search was not necessary, and the language model's response is returned as is.\n",
        "\n",
        "In essence:\n",
        "\n",
        "The custom_chain function implements the following logic:\n",
        "\n",
        "It takes a user question as input.\n",
        "\n",
        "It uses the query_analyzer to analyze the question and potentially generate a search query.\n",
        "\n",
        "If a search query is generated:\n",
        "\n",
        "It uses the retriever to perform a search.\n",
        "\n",
        "It returns the search results.\n",
        "\n",
        "If no search query is generated:\n",
        "\n",
        "It returns the language model's response directly."
      ],
      "metadata": {
        "id": "8HFVY26I0Y6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def custom_chain(question):\n",
        "    response = query_analyzer.invoke(question)\n",
        "    if \"tool_calls\" in response.additional_kwargs:\n",
        "        query = output_parser.invoke(response)\n",
        "        docs = retriever.invoke(query[0].query)\n",
        "        # Could add more logic - like another LLM call - here\n",
        "        return docs\n",
        "    else:\n",
        "        return response"
      ],
      "metadata": {
        "id": "Fbew4sM6fBes"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"where did Harrison Work\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-c3GcEzfDeD",
        "outputId": "81feef77-f809-4999-af27-28458d53b528"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a15875cd-41ea-41cf-8e23-e228a20e68d3', metadata={}, page_content='Harrison worked at Kensho')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output explanation\n",
        "\n",
        "This is a warning message from Chroma, the vector database.\n",
        "\"Number of requested results 4 is greater than number of elements in index 1\": This means that the retriever was asked to find the top 4 most similar documents (the default for many retrievers), but the Chroma vector store only contains 1 document.\n",
        "\"updating n_results = 1\": Chroma automatically adjusted the number of results to 1, since that's the maximum it can return.\n",
        "Essentially, it's telling you that your vector database is very small, and the retriever’s default setting is requesting more documents than exist."
      ],
      "metadata": {
        "id": "luWCIxOK1VYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi_6Io_4fHos",
        "outputId": "471ae2a1-70a9-4cce-f4a8-91eba3a739da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 93, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-33eada19-6f80-48eb-9e07-352003ba2b28-0', usage_metadata={'input_tokens': 93, 'output_tokens': 11, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_chain.invoke(\"Who is david !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOx34-MfPI8",
        "outputId": "bcad4be8-57f0-4c6c-b79d-e243c29e2a7a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Could you please provide more context about which David you are referring to? There are many notable individuals named David, including historical figures, celebrities, and fictional characters.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 95, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eab6ba8-fa97-4abc-8dfe-fda4ec97ee41-0', usage_metadata={'input_tokens': 95, 'output_tokens': 34, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6ynNXp-1dPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}