{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to add examples to the prompt for query analysis"
      ],
      "metadata": {
        "id": "8Uf2bpJcbGAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "WIgrgIrvSJJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-core langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5IM84g4R-SN",
        "outputId": "70e19912-ecb0-415f-d46a-2e6eda5eb954"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/414.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/414.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhiUI2TTSHDq",
        "outputId": "d8c4e31c-e574-4c86-b603-8bfd139dcfe9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define the Query schema\n",
        "\n",
        "We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a sub_queries field that contains more narrow questions derived from the top level question."
      ],
      "metadata": {
        "id": "PXBS1OwvSfNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. sub_queries_description String\n",
        "\n",
        "This string provides instructions for generating sub-queries.\n",
        "\n",
        "It explains that if the original question contains multiple distinct sub-questions or if there are generic questions that would be helpful to answer, a list of sub-questions should be created.\n",
        "\n",
        "It emphasizes that the list should be comprehensive, cover all parts of the original question, and that redundancy is acceptable.\n",
        "\n",
        "It also states that the sub-questions should be as narrowly focused as possible.\n",
        "\n",
        "3. Search Pydantic Model\n",
        "\n",
        "Search(BaseModel): Defines a Pydantic model called Search.\n",
        "\n",
        "query: str = Field(...): Specifies that the model has a field named query, which is a string.\n",
        "\n",
        "...: Indicates that this field is required.\n",
        "\n",
        "description: Provides a description of the field, which is used to explain the purpose of the field.\n",
        "\n",
        "sub_queries: List[str] = Field(...): Specifies that the model has a field named sub_queries, which is a list of strings.\n",
        "\n",
        "default_factory=list: Sets the default value of the field to an empty list.\n",
        "\n",
        "description: Uses the sub_queries_description string as the description for this field.\n",
        "\n",
        "publish_year: Optional[int] = Field(...): Specifies that the model has an optional field named publish_year, which is an integer.\n",
        "\n",
        "None: Sets the default value of the field to None.\n",
        "\n",
        "description: Provides a description of the field.\n",
        "\n",
        "Summary\n",
        "\n",
        "This code defines a Pydantic model that represents a search query for a database of tutorial videos. The model includes fields for the primary query, sub-queries, and the publication year. The sub_queries_description string provides instructions for generating sub-queries, which can be used to improve the search results."
      ],
      "metadata": {
        "id": "p3LRKiFIU0oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "sub_queries_description = \"\"\"\\\n",
        "If the original question contains multiple distinct sub-questions, \\\n",
        "or if there are more generic questions that would be helpful to answer in \\\n",
        "order to answer the original question, write a list of all relevant sub-questions. \\\n",
        "Make sure this list is comprehensive and covers all parts of the original question. \\\n",
        "It's ok if there's redundancy in the sub-questions. \\\n",
        "Make sure the sub-questions are as narrowly focused as possible.\"\"\"\n",
        "\n",
        "\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"Primary similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    sub_queries: List[str] = Field(\n",
        "        default_factory=list, description=sub_queries_description\n",
        "    )\n",
        "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
      ],
      "metadata": {
        "id": "aDOsZTEuSWy4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Query generation"
      ],
      "metadata": {
        "id": "ZsqCGRo3Sueq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. System Message\n",
        "\n",
        "This defines the system message for the LLM.\n",
        "\n",
        "It sets the LLM's role as an expert in converting user questions into database queries.\n",
        "\n",
        "It specifies that the LLM has access to a database of tutorial videos.\n",
        "\n",
        "It instructs the LLM to return a list of database queries optimized for retrieving relevant results.\n",
        "\n",
        "Key Instruction: It tells the LLM not to rephrase acronyms or unfamiliar words.\n",
        "\n",
        "3. Prompt Template\n",
        "\n",
        "This creates a ChatPromptTemplate with:\n",
        "\n",
        "(\"system\", system): The system message created earlier.\n",
        "\n",
        "MessagesPlaceholder(\"examples\", optional=True): A placeholder for examples, which can be dynamically inserted into the prompt. The optional=True argument means that the examples are not required.\n",
        "\n",
        "(\"human\", \"{question}\"): A human message with a placeholder for the user's question.\n",
        "\n",
        "4. LLM Initialization\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0): Initializes the OpenAI chat model with the \"gpt-4o-mini\" model and a temperature of 0 (making the output deterministic).\n",
        "\n",
        "structured_llm = llm.with_structured_output(Search): Creates a version of the LLM that is configured to return structured output in the form of the Search Pydantic model (defined in the previous explanation).\n",
        "\n",
        "5. query_analyzer Chain Creation\n",
        "\n",
        "{\"question\": RunnablePassthrough()}: This creates a dictionary that passes the \"question\" key from the input through to the next runnable in the chain.\n",
        "\n",
        "|: This is the LangChain \"pipe\" operator, which chains together runnables.\n",
        "\n",
        "prompt: The prompt template created earlier.\n",
        "\n",
        "structured_llm: The LLM configured to return structured output.\n",
        "\n",
        "Summary:\n",
        "\n",
        "This code sets up a LangChain chain that takes a user's question and uses an LLM to generate a structured search query. The LLM is configured to return the output in the form of a Search Pydantic model, which includes the primary query, sub-queries, and publication year. This allows for more structured and controlled interaction with the LLM, making it easier to parse and use the output in downstream applications."
      ],
      "metadata": {
        "id": "tykib5BBVohx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        MessagesPlaceholder(\"examples\", optional=True),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(Search)\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "id": "2A3tlObISoud"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssBxOAg3SzER",
        "outputId": "56beb307-74ac-4dd2-f0be-f4507e3edfb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='difference between web voyager and reflection agents', sub_queries=['What are web voyager agents?', 'What are reflection agents?', 'How do web voyager agents work?', 'How do reflection agents work?', 'Do both web voyager and reflection agents use langgraph?', 'What is langgraph?', 'What are the use cases for web voyager agents?', 'What are the use cases for reflection agents?'], publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Adding examples and tuning the prompt"
      ],
      "metadata": {
        "id": "En6_mHJ_TCLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "\n",
        "These examples are designed to be used as few-shot examples for a language model (LLM). They demonstrate how to convert natural language questions into structured search queries using the Search Pydantic model. By providing these examples to the LLM, you can help it learn to generate structured search queries for new questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "mQhqvNAJXiX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []"
      ],
      "metadata": {
        "id": "dUkgeXxQS8HL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's chat langchain, is it a langchain template?\"\n",
        "query = Search(\n",
        "    query=\"What is chat langchain and is it a langchain template?\",\n",
        "    sub_queries=[\"What is chat langchain\", \"What is a langchain template\"],\n",
        ")\n",
        "examples.append({\"input\": question, \"tool_calls\": [query]})"
      ],
      "metadata": {
        "id": "gNSguPt8TPqO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
        "query = Search(\n",
        "    query=\"How to build multi-agent system and stream intermediate steps from it\",\n",
        "    sub_queries=[\n",
        "        \"How to build multi-agent system\",\n",
        "        \"How to stream intermediate steps from multi-agent system\",\n",
        "        \"How to stream intermediate steps\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "examples.append({\"input\": question, \"tool_calls\": [query]})"
      ],
      "metadata": {
        "id": "knp31fjQTR3u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"LangChain agents vs LangGraph?\"\n",
        "query = Search(\n",
        "    query=\"What's the difference between LangChain agents and LangGraph? How do you deploy them?\",\n",
        "    sub_queries=[\n",
        "        \"What are LangChain agents\",\n",
        "        \"What is LangGraph\",\n",
        "        \"How do you deploy LangChain agents\",\n",
        "        \"How do you deploy LangGraph\",\n",
        "    ],\n",
        ")\n",
        "examples.append({\"input\": question, \"tool_calls\": [query]})"
      ],
      "metadata": {
        "id": "6P53KLAbTbi9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a tool_example_to_messages helper function to handle this for us"
      ],
      "metadata": {
        "id": "9mYXsYUTYce_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import necessary dependencies\n",
        "\n",
        "2. tool_example_to_messages Function:\n",
        "\n",
        "Purpose: Takes a dictionary (example) representing a conversation example and converts it into a list of BaseMessage objects.\n",
        "\n",
        "Input:\n",
        "\n",
        "example: A dictionary containing the conversation input (example[\"input\"]), tool calls (example[\"tool_calls\"]), and optionally tool outputs (example[\"tool_outputs\"]).\n",
        "\n",
        "Steps:\n",
        "\n",
        "Human Message: Creates a HumanMessage object from the example[\"input\"] and adds it to the messages list. This represents the user's input.\n",
        "\n",
        "Tool Call Conversion:\n",
        "\n",
        "Iterates through example[\"tool_calls\"].\n",
        "\n",
        "For each tool_call, it creates a dictionary (openai_tool_calls) that represents the tool call in a format compatible with OpenAI's tool calling API.\n",
        "\n",
        "It generates a unique ID using uuid.uuid4().\n",
        "\n",
        "It sets the type to \"function\".\n",
        "\n",
        "It extracts the tool's name from tool_call.__class__.__name__ (assuming tool_call is an object with a class name).\n",
        "\n",
        "It converts the tool's arguments to a JSON string using tool_call.json().\n",
        "\n",
        "The tool call information is appended to the openai_tool_calls list.\n",
        "\n",
        "AI Message with Tool Calls:\n",
        "\n",
        "Creates an AIMessage object with an empty content and adds the openai_tool_calls to its additional_kwargs. This represents the AI's response, which includes the tool calls.\n",
        "\n",
        "Tool Output Handling:\n",
        "\n",
        "Retrieves the tool_outputs from the example dictionary. If tool_outputs is not provided, it defaults to a list of \"You have correctly called this tool.\" messages, with a length that matches the number of tool calls.\n",
        "\n",
        "Iterates through the tool_outputs and openai_tool_calls lists in parallel.\n",
        "For each output and tool call, it creates a ToolMessage object, adding it to the messages list. This represents the result of the tool call.\n",
        "\n",
        "Return: Returns the messages list.\n",
        "\n",
        "3. example_msgs Creation:\n",
        "\n",
        "Purpose: Creates a flattened list of messages from a list of examples.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Uses a list comprehension to iterate through a list named examples (which is assumed to be defined elsewhere).\n",
        "\n",
        "For each ex in examples, it calls tool_example_to_messages(ex) to convert the example to a list of messages.\n",
        "\n",
        "It then flattens the resulting list of lists into a single list named example_msgs.\n",
        "\n",
        "Summary:\n",
        "\n",
        "This code is a utility function used to convert data from a specific format into a list of messages. This is common when using LangChain to train or test a language model that uses tools. The code effectively transforms a structured representation of a conversation into a sequence of messages that LangChain can understand and process."
      ],
      "metadata": {
        "id": "_4GTsJGSYlvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from typing import Dict\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "\n",
        "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
        "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
        "    openai_tool_calls = []\n",
        "    for tool_call in example[\"tool_calls\"]:\n",
        "        openai_tool_calls.append(\n",
        "            {\n",
        "                \"id\": str(uuid.uuid4()),\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool_call.__class__.__name__,\n",
        "                    \"arguments\": tool_call.json(),\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "    messages.append(\n",
        "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
        "    )\n",
        "    tool_outputs = example.get(\"tool_outputs\") or [\n",
        "        \"You have correctly called this tool.\"\n",
        "    ] * len(openai_tool_calls)\n",
        "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
        "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
        "    return messages\n",
        "\n",
        "\n",
        "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXF2i3hzTfmt",
        "outputId": "553b192d-4d4a-4cf6-e81a-a6b72c8fe2fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-58cddf997614>:23: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  \"arguments\": tool_call.json(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "This code defines a chain that:\n",
        "\n",
        "Receives a question as input (within a dictionary).\n",
        "\n",
        "Prepares a prompt that includes the question and a set of example messages.\n",
        "Sends the prepared prompt to a structured language model.\n",
        "\n",
        "Receives and returns the structured response from the language model.\n",
        "\n",
        "This is a common pattern for building question-answering or query-analysis systems that learn from examples. The examples provide context and guide the language model in generating appropriate responses."
      ],
      "metadata": {
        "id": "w4fpGYgLZWAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "query_analyzer_with_examples = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | prompt.partial(examples=example_msgs)\n",
        "    | structured_llm\n",
        ")"
      ],
      "metadata": {
        "id": "wyPfAizjTtfF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer_with_examples.invoke(\n",
        "    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T35iMDhmTzsL",
        "outputId": "0e98dadf-3f7b-4671-bbf5-2e747481716c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query=\"What's the difference between web voyager and reflection agents? Do both use langgraph?\", sub_queries=['What is web voyager?', 'What are reflection agents?', 'What is the difference between web voyager and reflection agents?', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer_with_examples.invoke(\n",
        "    \"what's the difference between Agentic RAG and Corrective RAG ?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_c-YUgBT3R3",
        "outputId": "b55a7a3d-370a-4da9-93ff-31ab63a5c83c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='What is the difference between Agentic RAG and Corrective RAG?', sub_queries=['What is Agentic RAG?', 'What is Corrective RAG?', 'How do Agentic RAG and Corrective RAG differ?'], publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer_with_examples.invoke(\n",
        "    \"what's the difference between RAG's and Chatbots ?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebbTcGqMZly2",
        "outputId": "035592c8-dc93-4508-9b2e-64ae5205cdc2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='What is the difference between RAGs and chatbots?', sub_queries=['What are RAGs?', 'What are chatbots?', 'How do RAGs work compared to chatbots?'], publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7jogrKeZwbf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}